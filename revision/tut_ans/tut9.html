<HTML>
<HEAD>
  <TITLE>COMP3411 Tutorial Solutions Week 9</TITLE>
</HEAD>
<style><!-- A {text-decoration:none} --></style>
<BODY ALINK="#660000" VLINK="#000066" LINK="#800066" BGCOLOR="#ffffff">

<H2><P><CENTER><B><FONT COLOR="#000066">
COMP3411/9414 Artificial Intelligence<br>
Session 1, 2019</FONT></B></CENTER></P></H2>

<H2><P><CENTER><B>Tutorial Solutions - Week 9</B></CENTER></P></H2>

<P><B><FONT SIZE=-1>
<SCRIPT LANGUAGE="JavaScript">
document.write("This page was last updated: " +
document.lastModified);
</SCRIPT>
</FONT>
</B>
</P>

<hr align=left>

<b>Activity 10.1</b> Conditional Probability
<hr>
<p>
Only 4% of the population are color blind,
but 7% of men are color blind.
What percentage of color blind people are men?
<blockquote>
If we assume 50% of the population are men,
then the fraction of "color blind men" is
0.5 * 0.07 = 0.035<br>
This means the fraction of "color blind women" is
0.04 - 0.035 = 0.005<br>
Therefore, the fraction of color blind people who are men
is 0.035 / 0.04 = 87.5%.
</blockquote>
<p>
<hr>
<b>Activity 10.2</b> Enumerating Probabilities
<hr>
<p>
(Exercise 13.8 from Russell &amp; Norvig)
<p>
<table border="1">
<tr><th></th><th colspan="2">toothache</th><th colspan="2">&not;toothache</th></tr>
<tr><th></th><th>catch</th><th>&not;catch</th><th>catch</th><th>&not;catch</th></tr>
<tr><th>&nbsp;cavity</th><td>&nbsp;.108</td><td>&nbsp;.012</td><td>&nbsp;.072</td><td>&nbsp;.008</td></tr>
<tr><th>&not;cavity</th><td>&nbsp;.016</td><td>&nbsp;.064</td><td>&nbsp;.144</td><td>&nbsp;.576</td></tr>
</table>
<p>
<ol type="1">
<li> Given the full joint distribution shown in Figure 13.3
(also on page 17 of the Uncertainty lecture slides), calculate
the following:
<p>
<ol type=a>
<li>
P( toothache &and; &not; catch ) = 0.012 + 0.064 = 0.076
<p>
<li> P( catch ) = 0.108 + 0.016 + 0.072 + 0.144 = 0.34
<p>
<li> P( cavity | catch ) = P( cavity &and; catch ) / P( catch )<br>
= ( 0.108 + 0.072 )/( 0.108 + 0.072 + 0.016 + 0.144 ) = 0.18 / 0.34 = 0.53
<p>
<li> P( cavity | toothache &or; catch ) =<br>
P( cavity &and; (toothache &or; catch)) / P( toothache &or; catch )<br>
= ( 0.108 + 0.012 + 0.072 )/( 0.108 + 0.012 + 0.072 + 0.016 + 0.064 + 0.144 )<br>
= 0.192 / 0.416 = 0.46
</ol>
<p>
<li> Verify the conditional independence claimed in the lecture slides
by showing that P( catch | toothache, cavity ) = P( catch | cavity )
<p>
P( catch | toothache &and; cavity ) =<br>
P( catch &and; toothach &and; cavity ) / P( toothache &and; cavity )<br>
= 0.108 /( 0.108 + 0.012 ) = 0.108 / 0.12 = 0.9
<p>
P( catch | cavity ) = P( catch &and; cavity ) / P( cavity )<br>
= ( 0.108 + 0.072 )/( 0.108 + 0.012 + 0.072 + 0.008 ) = 0.18 / 0.2 = 0.9<br>
</ol>
<p>
<hr>
<b>Activity 10.3</b> Wumpus World with Three Pits
<hr>
<blockquote>
<img src="./wk9_wumpus1.jpg" width="256">
</blockquote>
Consider the same Wumpus World situation shown above, but this
time making a different prior assumption about the placement of the
Pits - namely, that exactly three Pits have been placed
randomly among the sixteen squares at the beginning of the game. Using
this new prior, calculate:
<ol type="1">
<li> the probability of a Pit in (1,3)
<li> the probability of a Pit in (2,2)
</ol>
<blockquote>
We start with these equations from the Lecture Notes:
</blockquote>
<i>P(Pit<sub>1,3</sub> | known) = &Sigma;<sub>fringe &isin; F</sub> P(Pit<sub>1,3</sub> | fringe)
&Sigma;<sub>other</sub> P(known | fringe,other)P(fringe,other) / P(known)</i>
<p>
<i>P(known) = &Sigma;<sub>fringe &isin; F</sub> &Sigma;<sub>other</sub> P(known | fringe,other) P(fringe,other)</i>
<blockquote>
<img src="./wk9_wumpus2.jpg" width="512">
<p>
In this case,
<i>P(known | fringe,other) = 1</i>
if and only if <i>fringe &isin; F</i> and <i>fringe, other</i>
between them contain exactly three Pits. For these configurations,
<i>P(fringe,other)</i> becomes a constant that can
be cancelled from the numerator and denominator.
If there are two pits in the fringe, the other pit can be placed in 10
different squares. If there is only one pit in the fringe, the other
two pits can be placed in 45 different ways. Therefore,
<p>
<i>P(Pit<sub>1,3</sub> | known)</i> = (1+10+10)/(1+10+10+10+45) = 21/76 &asymp; 0.276
<p>
<i>P(Pit<sub>2,2</sub> | known)</i> = (1+10+10+45)/(1+10+10+10+45) = 66/76 &asymp; 0.868
</blockquote>
<p>

<hr>

<b>Activity 9.4</b> Q-Learning
<hr>
<p>
Consider a world with two states
S = {S<sub>1</sub>, S<sub>2</sub>}
and two actions
A = {a<sub>1</sub>, a<sub>2</sub>},
where the transitions &delta; and reward <em>r</em>
for each state and action are as follows:
<blockquote>
<table>
<tr align=right><td>&delta;(S<sub>1</sub>, a<sub>1</sub>) = S<sub>1</sub>,</td>
 <td><em>r</em>(S<sub>1</sub>, a<sub>1</sub>) =</td><td>0</td></tr>
<tr align=right><td>&delta;(S<sub>1</sub>, a<sub>2</sub>) = S<sub>2</sub>,</td>
 <td><em>r</em>(S<sub>1</sub>, a<sub>2</sub>) =</td><td>-1</td></tr>
<tr align=right><td>&delta;(S<sub>2</sub>, a<sub>1</sub>) = S<sub>2</sub>,</td>
 <td><em>r</em>(S<sub>2</sub>, a<sub>1</sub>) =</td><td>+1</td></tr>
<tr align=right><td>&delta;(S<sub>2</sub>, a<sub>2</sub>) = S<sub>1</sub>,</td>
 <td><em>r</em>(S<sub>2</sub>, a<sub>2</sub>) =</td><td>+5</td></tr>
</table>
</blockquote>

<ol type="1">

<li>
Draw a picture of this world, using circles for the states and
arrows for the transitions.
<p>
<img src="wk9_qlearn.jpg">
<p>

<li>
Assuming a discount factor of &gamma; = 0.9, determine:
<ol type=a>
<li>the optimal policy &pi;<sup>*</sup> : S &rarr; A
<blockquote>
&pi;<sup>*</sup>(S<sub>1</sub>) = a<sub>2</sub><br>
&pi;<sup>*</sup>(S<sub>2</sub>) = a<sub>2</sub>
</blockquote>

<li>the value function V<sup>*</sup> : S &rarr; R
<blockquote>
V(S<sub>1</sub>) = -1 + &gamma;V(S<sub>2</sub>)<br>
V(S<sub>2</sub>) =  5 + &gamma;V(S<sub>1</sub>)
<p>
So V(S<sub>1</sub>) = -1 + 5&gamma; + &gamma;<sup>2</sup> V(S<sub>1</sub>)<br>
i.e. V(S<sub>1</sub>)
= (-1 + 5&gamma;) / (1 - &gamma;<sup>2</sup>)
= 3.5 / 0.19 = 18.42
<p>
V(S<sub>2</sub>) = 5 + &gamma;V(S<sub>1</sub>)
= 5 + 0.9 * 3.5 / 0.19 = 21.58
</blockquote>

<li>the "Q" function Q : S x A &rarr; R
<blockquote>
<table>
<tr><td>Q(S<sub>1</sub>, a<sub>1</sub>) =</td>
<td align=right>&gamma;V(S<sub>1</sub>) = 16.58</td></tr>
<tr><td>Q(S<sub>1</sub>, a<sub>2</sub>) =</td>
<td align=right>V(S<sub>1</sub>) = 18.42</td></tr>
<tr><td>Q(S<sub>2</sub>, a<sub>1</sub>) =</td>
<td align=right>1 + &gamma;V(S<sub>2</sub>) = 20.42</td></tr>
<tr><td>Q(S<sub>2</sub>, a<sub>2</sub>) =</td>
<td align=right>V(S<sub>2</sub>) = 21.58</td></tr>
</table>
</blockquote>
</ol>
<p>

<li>
Write the Q values in a matrix:
<p>
<table border=1>
<tr><th>Q</td><th>a<sub>1</sub></th><th>a<sub>2</sub></th></tr>
<tr align=right><th>S<sub>1</sub></th><td>&nbsp;16.58&nbsp;</td><td>&nbsp;18.42&nbsp;</td></tr>
<tr align=right><th>S<sub>2</sub></th><td>&nbsp;20.42&nbsp;</td><td>&nbsp;21.58&nbsp;</td></tr>
</table>
</blockquote>

<li>
Trace through the first few steps of the Q-learning algorithm,
with all Q values initially set to zero.
Explain why it is necessary to force exploration through
probabilistic choice of actions, in order to ensure
convergence to the true Q values.
<blockquote>
<table border=1>
<tr><th>current state</th><th>chosen action</th><th>new Q value</th></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>1</sub></td><td>0 + &gamma;*0 = 0</td></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>2</sub></td><td>-1 + &gamma;*0 = -1</td></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>1</sub></td><td>1 + &gamma;*0 = +1</td></tr>
</table>
</blockquote>
<p>
At this point, the table looks like this:
<p>
<blockquote>
<table border=1>
<tr><th>Q</td><th>a<sub>1</sub></th><th>a<sub>2</sub></th></tr>
<tr align=right><th>S<sub>1</sub></th><td>&nbsp;0&nbsp;</td><td>&nbsp;-1&nbsp;</td></tr>
<tr align=right><th>S<sub>2</sub></th><td>&nbsp;1&nbsp;</td><td>&nbsp;0&nbsp;</td></tr>
</table>
</blockquote>
<p>
If we do not force exploration, the agent will always
prefer action a<sub>1</sub> in state S<sub>2</sub>,
so it will never explore action a<sub>2</sub>.
This means that Q(S<sub>2</sub>, a<sub>2</sub>) will remain zero
forever, instead of converging to the true value of 21.58 .
If we force exploration, the next few steps might look like this:
<p>
<blockquote>
<table border=1>
<tr><th>current state</th><th>chosen action</th><th>new Q value</th></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>2</sub></td><td>5 + &gamma;*0 = 5</td></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>1</sub></td><td>0 + &gamma;*0 = 0</td></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>2</sub></td><td>-1 + &gamma;*5 = 3.5</td></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>1</sub></td><td>1 + &gamma;*5 = 5.5</td></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>2</sub></td><td>5 + &gamma;*3.5 = 8.15</td></tr>
</table>
</blockquote>
<p>
Now we have this table:
<p>
<blockquote>
<table border=1>
<tr><th>Q</td><th>a<sub>1</sub></th><th>a<sub>2</sub></th></tr>
<tr align=right><th>S<sub>1</sub></th><td>&nbsp;0&nbsp;</td><td>&nbsp;3.5&nbsp;</td></tr>
<tr align=right><th>S<sub>2</sub></th><td>&nbsp;5.5&nbsp;</td><td>&nbsp;8.15&nbsp;</td></tr>
</table>
</blockquote>
<p>
From this point on, the agent will prefer action a<sub>2</sub>
both in state S<sub>1</sub> and in state S<sub>2</sub>.
Further steps refine the Q value estimates, and, in the limit,
they will converge to their true values.
<p>
<blockquote>
<table border=1>
<tr><th>current state</th><th>chosen action</th><th>new Q value</th></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>1</sub></td><td>0 + &gamma;*3.5 = 3.15</td></tr>
<tr align=center><td>S<sub>1</sub></td><td>a<sub>2</sub></td><td>-1 + &gamma;*8.15 = 6.335</td></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>1</sub></td><td>1 + &gamma;*8.15 = 8.335</td></tr>
<tr align=center><td>S<sub>2</sub></td><td>a<sub>2</sub></td><td>5 + &gamma;*6.34 = 10.70</td></tr>
</table>
&nbsp;&nbsp;etc...
</blockquote>
</ol>
<p>

<hr align=left>
</body>
</html>
